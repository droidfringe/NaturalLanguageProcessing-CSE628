Name - Gaurav Ahuja
SBU Id - 111500318

Implementation Details:

1. Generate Batch:
For generating training batch, num_skips/2 words to left and right of context word were chosen.
When the context word reached the end of data, it was reset to first word in data.

2. Cross Entropy Loss:
For each training example, computed the numerator of cross entropy loss by doing dot product of U and V.
For computing the denominator, multiplied each vector of U with each vector of V (V*Ut).
Summation of each row of VUt was taken to comoute the denominator for each example.

3. NCE Loss:
In NCE loss, the vector of context word had interactions involving two terms, one was outer words and the other were negative samples.
So, the NCE loss computation was done in 2 parts.
Embeddings and bias for outer words were extracted and their dot product was taken with context word embeddings.
To compute the second term, each vector of V (context words) was multiplied with each vector of negative samples(Un) and bias and unigram probability terms were added.
Summation of each row was computed to get the second term in NCE loss.
Finally, -(sum of the two terms) was computed as NCE loss.

4. Word Analogy Task:
A relation R was represented by a vector, which was computed by taking average of difference vectors between given word pairs.
Cosine similarity was used to quantify the similarit y of a word pair with R.
Most and least illustrative pairs were computed using Cosine similarity.

5. Experiments:
The following hyperparameter settings were explored for the following loss functions:

For cross entropy loss:
batch_sizes = [64, 128, 256]
window_params (skip_window, num_skips) = [(2, 4), (4, 8), (8, 16)]
Total = 3x3 = 9 configurations

For NCE loss:
batch_sizes = [64, 128, 256]
window_params (skip_window, num_skips) = [(2, 4), (4, 8), (8, 16)]
num_negative_samples = [32, 64, 128]
Total = 3x3x3 = 27 configurations

For each setting, accuracy on training data was calculated and the settings which gave best result for each loss function were used to compute the test set results.
The results are shown in report.

Best model parameters:

Cross Entropy - batch size = 128, skip_window = 4, num_skips = 8
NCE - batch size = 128, skip_window = 4, num_skips = 8, num_sampled = 32